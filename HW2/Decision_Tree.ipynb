{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the entropy of a subset\n",
    "def entropy(target_col):\n",
    "    labels, counts = np.unique(target_col, return_counts=True)\n",
    "    counts = list(counts)\n",
    "    return np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts))for i in range(len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the information gain of a dataset\n",
    "def InfoGain(data, split_feature, target_feature=\"Label\"):\n",
    "    pre_entropy = entropy(data[target_feature])\n",
    "    \n",
    "    # Get unique values in the split feature and their corresponding counts\n",
    "    vals, counts = np.unique(data[split_feature], return_counts=True)\n",
    "    counts = list(counts)\n",
    "    post_entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_feature]==vals[i]).dropna()[target_feature]) for i in range(len(vals))])\n",
    "    #print(\"Entropy: \",pre_entropy, post_entropy)\n",
    "    return pre_entropy - post_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GainRatio(data, split_feature, target_feature=\"Label\"):\n",
    "    info_gain = InfoGain(data, split_feature, target_feature)\n",
    "#     print(\"Info gain\",info_gain)\n",
    "    vals, counts = np.unique(data[split_feature], return_counts=True)\n",
    "    counts = list(counts)\n",
    "    #print(counts)\n",
    "    #print(\"infos: \",[-(counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(counts))])\n",
    "    split_info = np.sum([-(counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(counts))])\n",
    "#     print(vals)\n",
    "#     print(\"Split infos\", [-(counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(counts))])\n",
    "    return info_gain/split_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GiniIndex(target_col):\n",
    "    labels, counts = np.unique(target_col, return_counts=True)\n",
    "    counts = list(counts)\n",
    "    return 1-np.sum([np.square((-counts[i]/np.sum(counts))) for i in range(len(labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reduction_Gini(data, split_feature, target_feature=\"Label\"):\n",
    "    pre_gini = GiniIndex(data[target_feature])\n",
    "    vals, counts = np.unique(data[split_feature], return_counts=True)\n",
    "    counts = list(counts)\n",
    "    post_gini = np.sum([counts[i]/np.sum(counts)*GiniIndex(data.where(data[split_feature]==vals[i]).dropna()[target_feature]) for i in range(len(counts))])\n",
    "    #print(pre_gini, post_gini)\n",
    "    return pre_gini-post_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ID3 Algorithm: uses infomation as as splitting criteria\n",
    "def ID3(data, original_data, cur_features,Label):\n",
    "    tree = {}    \n",
    "    info_gains = [InfoGain(data, sp) for sp in cur_features]\n",
    "    #print(cur_features)\n",
    "    #print(info_gains)\n",
    "    if len(info_gains) > 0:\n",
    "        best_feature_index = np.argmax(info_gains)\n",
    "        best_feature = cur_features[best_feature_index]\n",
    "        best_feature_vals = np.unique(data[best_feature])\n",
    "    if data.empty:\n",
    "        mode = np.unique(original_data[Label])[np.argmax(np.unique(original_data[Label],return_counts=True)[1])]\n",
    "        return(mode)        \n",
    "    elif len(np.unique(data[Label])) == 1:\n",
    "        return np.unique(data[Label])[0]\n",
    "    elif len(cur_features) == 0 or all(infogain==0 for infogain in list(info_gains)):\n",
    "        vals,counts = np.unique(data[Label],return_counts=True)\n",
    "        if all(count==counts[0] for count in counts):\n",
    "            mode = np.unique(original_data[Label])[np.argmax(np.unique(original_data[Label],return_counts=True)[1])]\n",
    "        else:\n",
    "            mode = vals[np.argmax(counts)]\n",
    "        return(mode)\n",
    "    all_feature_vals = np.unique(original_data[best_feature])\n",
    "    sub_datas = [data.where(data[best_feature]==val).dropna().drop(columns=best_feature) \n",
    "                 if val in list(data[best_feature]) else pd.DataFrame() for val in all_feature_vals]\n",
    "    del cur_features[best_feature_index]\n",
    "    for i,sub_data in enumerate(sub_datas):\n",
    "        tree[all_feature_vals[i]] = ID3(sub_data, original_data, cur_features[:],Label)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4.5 Algorithm: uses gain ratio as as splitting criteria\n",
    "def C45(data, original_data, cur_features,Label):\n",
    "    if data.empty:\n",
    "        mode = np.unique(original_data[Label])[np.argmax(np.unique(original_data[Label],return_counts=True)[1])]\n",
    "        return(mode)  \n",
    "    \n",
    "    tree = {}    \n",
    "    gain_ratios = [GainRatio(data, sp) for sp in cur_features]\n",
    "    #print(\"Current features:\",cur_features)\n",
    "    #print(gain_ratios)\n",
    "    if len(gain_ratios) > 0:\n",
    "        best_feature_index = np.argmax(gain_ratios)\n",
    "        best_feature = cur_features[best_feature_index]\n",
    "        best_feature_vals = np.unique(data[best_feature])      \n",
    "    if len(np.unique(data[Label])) == 1:\n",
    "        return np.unique(data[Label])[0]\n",
    "    elif len(cur_features) == 0 or all(gainratio==0 for gainratio in list(gain_ratios)):\n",
    "        vals,counts = np.unique(data[Label],return_counts=True)\n",
    "        if all(count==counts[0] for count in counts):\n",
    "            mode = np.unique(original_data[Label])[np.argmax(np.unique(original_data[Label],return_counts=True)[1])]\n",
    "        else:\n",
    "            mode = vals[np.argmax(counts)]\n",
    "        return(mode)\n",
    "    all_feature_vals = np.unique(original_data[best_feature])\n",
    "    sub_datas = [data.where(data[best_feature]==val).dropna().drop(columns=best_feature) \n",
    "                 if val in list(data[best_feature]) else pd.DataFrame() for val in all_feature_vals]\n",
    "    del cur_features[best_feature_index]\n",
    "    for i,sub_data in enumerate(sub_datas):\n",
    "        tree[all_feature_vals[i]] = C45(sub_data, original_data, cur_features[:],Label)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CART Algorithm: uses gini index as as splitting criteria\n",
    "def CART(data, original_data, cur_features,Label):\n",
    "    if data.empty:\n",
    "        mode = np.unique(original_data[Label])[np.argmax(np.unique(original_data[Label],return_counts=True)[1])]\n",
    "        return(mode)  \n",
    "    tree = {}    \n",
    "    ginis = [Reduction_Gini(data, sp) for sp in cur_features]\n",
    "    #print(\"Current features:\",cur_features)\n",
    "    #print(ginis)\n",
    "    if len(ginis) > 0:\n",
    "        best_feature_index = np.argmax(ginis)\n",
    "        best_feature = cur_features[best_feature_index]\n",
    "        best_feature_vals = np.unique(data[best_feature])      \n",
    "    if len(np.unique(data[Label])) == 1:\n",
    "        return np.unique(data[Label])[0]\n",
    "    elif len(cur_features) == 0 or all(gini==0 for gini in list(ginis)):\n",
    "        vals,counts = np.unique(data[Label],return_counts=True)\n",
    "        if all(count==counts[0] for count in counts):\n",
    "            mode = np.unique(original_data[Label])[np.argmax(np.unique(original_data[Label],return_counts=True)[1])]\n",
    "        else:\n",
    "            mode = vals[np.argmax(counts)]\n",
    "        return(mode)\n",
    "    all_feature_vals = np.unique(original_data[best_feature])\n",
    "    sub_datas = [data.where(data[best_feature]==val).dropna().drop(columns=best_feature) \n",
    "                 if val in list(data[best_feature]) else pd.DataFrame() for val in all_feature_vals]\n",
    "    del cur_features[best_feature_index]\n",
    "    for i,sub_data in enumerate(sub_datas):\n",
    "        tree[all_feature_vals[i]] = CART(sub_data, original_data, cur_features[:],Label)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def DecisionTreeClassifier(data, originalTree, Label=True):\n",
    "    results = []\n",
    "    if Label:\n",
    "        features= list(train)[1:-1]\n",
    "        Label = list(train)[-1]\n",
    "    else:\n",
    "        features= list(train)[1:-1]\n",
    "    \n",
    "    for i,row in data[features].iterrows():\n",
    "        #print(\"Original:\",originalTree)\n",
    "\n",
    "        dtree = originalTree\n",
    "        #print(row.keys())\n",
    "        vals = list([row[key] for key in row.keys()])\n",
    "        found = False\n",
    "        while len(vals) > 0 and not found:\n",
    "            #print(vals)\n",
    "            #print(\"The tree right now:\",dtree)\n",
    "            for val in vals:\n",
    "                if val in dtree.keys():\n",
    "                    ind = vals.index(val)\n",
    "                    #print(\"vals:\",vals)\n",
    "                    #print(\"val:\",val)\n",
    "                    #print(\"index:\",ind)\n",
    "                    vals.remove(val)\n",
    "                    if not isinstance(dtree[val], dict):\n",
    "                        #print(dtree[val])\n",
    "                        results.append(dtree[val])\n",
    "                        found = True\n",
    "                    else:\n",
    "                        dtree = dtree[val]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3 Tree: {'In': 'Lose', 'Out': 'Win'}\n",
      "C45 Tree: {'In': 'Lose', 'Out': 'Win'}\n",
      "CART Tree: {'In': 'Lose', 'Out': 'Win'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yunch\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "########### Question 1 ###############\n",
    "train = pd.read_csv('q1_train.csv')\n",
    "features= list(train)[1:-1]\n",
    "Label = list(train)[-1]\n",
    "tree_ID3 = ID3(train,train,features[:],Label)\n",
    "tree_C45 = C45(train,train,features[:],Label)\n",
    "tree_CART = CART(train,train,features[:],Label)\n",
    "print(\"ID3 Tree:\", tree_ID3)\n",
    "print(\"C45 Tree:\",tree_C45)\n",
    "print(\"CART Tree:\",tree_CART)\n",
    "########### Question 1 ###############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3 Tree: {'Overcast': 'Yes', 'Rainy': {False: 'Yes', True: 'No'}, 'Sunny': {'High': 'No', 'Normal': 'Yes'}}\n",
      "C45 Tree: {'Overcast': 'Yes', 'Rainy': {False: 'Yes', True: 'No'}, 'Sunny': {'High': 'No', 'Normal': 'Yes'}}\n",
      "CART Tree: {'Overcast': 'Yes', 'Rainy': {False: 'Yes', True: 'No'}, 'Sunny': {'High': 'No', 'Normal': 'Yes'}}\n"
     ]
    }
   ],
   "source": [
    "########### Question 1 ###############\n",
    "train = pd.read_csv('q2_train.csv')\n",
    "features= list(train)[1:-1]\n",
    "Label = list(train)[-1]\n",
    "tree_ID3 = ID3(train,train,features[:],Label)\n",
    "tree_C45 = C45(train,train,features[:],Label)\n",
    "tree_CART = CART(train,train,features[:],Label)\n",
    "print(\"ID3 Tree:\", tree_ID3)\n",
    "print(\"C45 Tree:\",tree_C45)\n",
    "print(\"CART Tree:\",tree_CART)\n",
    "########### Question 1 ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ABC': {'In': 'Lose', 'Out': 'Win'}, 'CBS': 'Lose', 'ESPN': 'Win', 'FOX': 'Lose', 'NBC': {'In': 'Win', 'Out': {'Away': 'Win', 'Home': 'Win'}}}\n",
      "{'In': {'Away': 'Lose', 'Home': {'ABC': 'Win', 'CBS': 'Win', 'ESPN': 'Win', 'FOX': 'Win', 'NBC': 'Win'}}, 'Out': {'ABC': {'Away': 'Win', 'Home': 'Win'}, 'CBS': 'Lose', 'ESPN': 'Win', 'FOX': 'Win', 'NBC': {'Away': 'Win', 'Home': 'Win'}}}\n",
      "{'ABC': {'In': 'Lose', 'Out': 'Win'}, 'CBS': 'Lose', 'ESPN': 'Win', 'FOX': 'Lose', 'NBC': {'In': 'Win', 'Out': {'Away': 'Win', 'Home': 'Win'}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yunch\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('q5_train.csv')\n",
    "test = pd.read_csv('q5_test.csv')\n",
    "true_labels = list(test[\"Label\"])\n",
    "\n",
    "# Get feature names and label name\n",
    "features= list(train)[1:-1]\n",
    "Label = list(train)[-1]\n",
    "\n",
    "tree_ID3 = ID3(train,train,features[:],Label)\n",
    "tree_C45 = C45(train,train,features[:],Label)\n",
    "tree_CART = CART(train,train,features[:],Label)\n",
    "print(tree_ID3)\n",
    "print(tree_C45)\n",
    "print(tree_CART)\n",
    "\n",
    "predicted_ID3 = DecisionTreeClassifier(test, tree_ID3)\n",
    "predicted_C45 = DecisionTreeClassifier(test, tree_C45)\n",
    "predicted_CART = DecisionTreeClassifier(test, tree_CART)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(true_labels)\n",
    "true_labels = le.transform(true_labels)\n",
    "predicted_ID3 = le.transform(predicted_ID3)\n",
    "predicted_C45 = le.transform(predicted_C45)\n",
    "predicted_CART = le.transform(predicted_CART)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "test_X = []\n",
    "le = [preprocessing.LabelEncoder().fit(list(train[features[i]])) for i in range(len(features))]\n",
    "for i, row in train[features].iterrows():\n",
    "    train_X.append([le[j].transform([list(row)[j]])[0] for j in range(len(list(row)))])\n",
    "for i, row in test[features].iterrows():\n",
    "    test_X.append([le[j].transform([list(row)[j]])[0] for j in range(len(list(row)))])\n",
    "\n",
    "y = list(train[Label])\n",
    "le = preprocessing.LabelEncoder().fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(np.array(train_X), y)\n",
    "predicted_NB = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results with ID3 Classification Model:\n",
      "Accuracy:  0.8333333333333334\n",
      "Precision:  0.8888888888888888\n",
      "Recall:  0.8888888888888888\n",
      "F1:  0.8888888888888888\n",
      "\n",
      "Test results with C45 Classification Model:\n",
      "Accuracy:  0.9166666666666666\n",
      "Precision:  0.9\n",
      "Recall:  1.0\n",
      "F1:  0.9473684210526316\n",
      "\n",
      "Test results with Naive Bayes Classification Model:\n",
      "Accuracy:  0.75\n",
      "Precision:  0.75\n",
      "Recall:  1.0\n",
      "F1:  0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"Test results with ID3 Classification Model:\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(true_labels, predicted_ID3))\n",
    "print(\"Precision: \", metrics.precision_score(true_labels, predicted_ID3))\n",
    "print(\"Recall: \", metrics.recall_score(true_labels, predicted_ID3))\n",
    "print(\"F1: \", metrics.f1_score(true_labels, predicted_ID3))\n",
    "print()\n",
    "print(\"Test results with C45 Classification Model:\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(true_labels, predicted_C45))\n",
    "print(\"Precision: \", metrics.precision_score(true_labels, predicted_C45))\n",
    "print(\"Recall: \", metrics.recall_score(true_labels, predicted_C45))\n",
    "print(\"F1: \", metrics.f1_score(true_labels, predicted_C45))\n",
    "print()\n",
    "print(\"Test results with Naive Bayes Classification Model:\")\n",
    "print(\"Accuracy: \", metrics.accuracy_score(true_labels, predicted_NB))\n",
    "print(\"Precision: \", metrics.precision_score(true_labels, predicted_NB))\n",
    "print(\"Recall: \", metrics.recall_score(true_labels, predicted_NB))\n",
    "print(\"F1: \", metrics.f1_score(true_labels, predicted_NB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
